[
{
	"uri": "/",
	"title": "Confluent Kafka Horizontal Workshop - FSI",
	"tags": [],
	"description": "",
	"content": " Confluent Kafka Workshop Welcome In this workshop you will learn why DevOps is a pattern for modernization and how to use patterns to release features and functionality at a faster pace.\nLearning Objectives  Deploy CloudFormation for completing workshop exercises. How DevOps patterns help to release features and functionality faster. Best practices for implementing 2 pizza team pipelines.  The examples and sample code provided in this workshop are intended to be consumed as instructional content. These will help you understand how various AWS services can be architected to build a solution while demonstrating best practices along the way. These examples are not intended for use in production environments.  "
},
{
	"uri": "/1.-introduction.html",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Introduction to Confluent Horizontal Workshop - FSI Introduction Making decisions in near-realtime is a challenge many of us are looking to solve. Confluent offers a number of data connectors and simplified stream processing that can make this less complicated and more reliable. During this workshop you will learn how to use fully-managed Confluent Cloud on AWS to source historical data into Kafka, use ksqlDB to process credit applications in real time using the sourced historical data together with generated events. Once our streaming applications is working we will sink the approved and not approved events into Amazon Redshift and finally we will visualize this data using Amazon QuickSight.\n"
},
{
	"uri": "/2.-prerequisites.html",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": " Prerequisites  You need to have an active AWS account. If you don’t have one,create and activate a new AWS account. You need to have an active Confluent Cloud account. [Link to Next page]  "
},
{
	"uri": "/3.-signing-up-for-confluent-cloud.html",
	"title": "Signing up for Confluent Cloud",
	"tags": [],
	"description": "",
	"content": " Signing up for Confluent Cloud If you don’t already have an active Confluence Cloud account, navigate to AWS Marketplace and find Apache Kafka on Confluent Cloud – Pay As You Go. Select the Continue to Subscribe button.\nOn the next page, review the pricing details and select the Subscribe button.\nNext, to create an account, select the Set Up Your Account button on the pop-up window.\nYou will be redirected to a Confluent Cloud page. Provide the required details and create your account. Once created, you can use this account to carry out the remainder of this procedure. Note that when you sign up, you get a monthly $200.00 USD credit for the first three months.\n"
},
{
	"uri": "/4.-creating-a-confluent-cloud-kafka-cluster.html",
	"title": "Creating a Confluent Cloud Kafka Cluster",
	"tags": [],
	"description": "",
	"content": " Creating a Confluent Cloud Kafka Cluster Once you Log in, you will see a pop-up asking few details regarding your Kafka experience and roles. Click skip and go ahead.\nOn the next page, it will show options to configure your Confluent Cloud Kafka Cluster. Select Cluster Type as Basic and click Begin Configurations\nOn the next page, Select AWS and choose Region as any from the below 3 and Availability as Single zone for the workshop and click Continue:\n   Region us-west-2 us-east-2 us-east-1      On the next page, you will be asked to provide a credit card number to keep on file, provide that and click Review. But note that you will have $200 USD free usage balance which will be more than enough to complete this workshop and hence you won’t be charged.\nClick Review and proceed to next page. On the next page, provide a name for your cluster, say Confluent Workshop and click Launch cluster. The cluster will be created instantly.\nwithin a few seconds you will have a Confluent Cloud Kafka cluster up and running. Lets proceed with next steps\n"
},
{
	"uri": "/5.-creating-cluster-api-keys.html",
	"title": "Creating Cluster API Keys",
	"tags": [],
	"description": "",
	"content": " Creating Cluster API Keys Navigate to API Keys section and click on Create key, select the scope as Global and click Next.\nOn the next screen, you will see the keys generated for you. Ensure you save/copy the Key and Secret for use in the later sections. Provide a description, mark the check box and click Save\nOnce you have noted down the keys, proceed to next step.\n"
},
{
	"uri": "/6.-creating-confluent-cloud-schema-registry.html",
	"title": "Creating Confluent Cloud Schema registry",
	"tags": [],
	"description": "",
	"content": " Creating Confluent Cloud Schema registry On the left bottom, click on “Schema registry”\nClick Set up on my own and on the next page, select AWS and Region as US then click continue.\nNow the schema registry is created and you could see the API endpoint listed on the UI.\nWith this step, you have created all the confluent cloud resources needed for completing the workshop. But before we proceed to next step, lets note down the bootstrap URL for the cluster.\nWhile you are in the Cluster Page, Select Settings from left panel and note down/copy the Bootstrap server URL, you would need it during next section.\nNow you have Bootstrap server URL, API Key and API Secret handy to proceed with next section, where you will pass those as inputs for CloudFormation Deployments in the later step.\n"
},
{
	"uri": "/7.-creating-a-ksqldb-application.html",
	"title": "Creating a ksqlDB Application",
	"tags": [],
	"description": "",
	"content": " Creating a ksqlDB Application While still in Confluent User interface, click on ksqlDB on the left bottom of the panel. On the next page, Click on Create application myself\nOn the next page, Select Global Access for access control and click continue:\nProvide a name for the application, say aws_app, leave the Number of streaming units as 4, which is default and click Launch Application\nYou will notice that the application is in Provisioning state and it might take few minutes for it to be ready.\nMeanwhile lets complete other steps.\n"
},
{
	"uri": "/8.-creating-aws-resources.html",
	"title": "Creating AWS resources required",
	"tags": [],
	"description": "",
	"content": " Launching CloudFormation to create AWS resources needed for the Workshop Now while logged into your AWS account, click on the Launch Stack button below on the region you have selected while creating Confluent Cloud cluster.\nIt is very important to deploy CloudFormation and creating AWS resources in the same region as Confluent Cloud Cluster for this workshop to function properly.     us-west-2 us-east-2 us-east-1           When clicked on the launch stack button, it will take you to CloudFormation create Stack page, Provide API Key, Api Secret and BootStrap Server , acknowledge the capabilities and click Create Stack button.\nThis will start creating the AWS resources required and will take about 10minutes to complete the creation.\nOnce completed, navigate to the outputs tab and make a note of ApiGatewayInvokeURL, MySqlEndpoint, RedshiftEndpoint and keep them handy for using in following sections: Navigate to Resources Tab and click on GatewayApiKey physical id\nIt will take you to the GatewayAPIKey resource, click on the show button near API Key to reveal the key, and make a note of it to use in the next session.\nOnce you have noted down the GatewayAPIKey, ApiGatewayInvokeURL, MySqlEndpoint and RedshiftEndpoint, lets navigate back to Confluent Cloud UI for next steps.\n"
},
{
	"uri": "/9.-creating-a-kafka-topic.html",
	"title": "Creating a Kafka Topic ",
	"tags": [],
	"description": "",
	"content": " Creating a Kafka Topic Navigate to Confluent cloud UI and click on Topic on the left navigation bar. On the Topics page, click con Create Topic button.\nOn the next screen provide topic name as credit_applications, Number of Partitions as 1 and click Create with Defaults button. Topic will be created in few seconds.\nThis section is complete and lets move forward to complete next.\n"
},
{
	"uri": "/10.-configuring-mysql-source-connector.html",
	"title": "Configuring MySQL Source Connector",
	"tags": [],
	"description": "",
	"content": " Configuring MySQL Source Connector While on the Confluent cloud UI and click on Connector on the left panel, filter MySQL and click on MySQL source connector\nOn the Next page, provide below details: (while entering mysql table names, make sure you press enter after each value is inserted)\n   Key Value     Name LegacyMySqlSource   Kafka API Key Your Kafka API Key   Kafka API Secret Your Kafka API Secret   Topic prefix mysql_   Connection Host MySQL host from AWS CloudFormation output   Connection port 3306   Connection user admin   Connection password Password123   Database name confluentdemo   SSL mode prefer   Table names credit_payment_history, credit_utilization   Timestamp column name date   Table types TABLE   Database timezone America/Los_Angeles   Output message format JSON   Tasks 1    Then Click next. On the next page, review the details provided and click Launch. It would take few minutes for the connector to complete provisioning and start.\nYou will notice two topics created mysql_credit_payment_history and mysql_credit_utilization\nNow, lets move to next section.\n"
},
{
	"uri": "/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]